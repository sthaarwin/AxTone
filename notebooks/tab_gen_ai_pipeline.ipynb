{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955bdda3",
   "metadata": {},
   "source": [
    "# Tab Generation AI Pipeline Experiment\n",
    "\n",
    "This notebook provides a step-by-step workflow for experimenting with the tab-gen-ai pipeline, demonstrating how to transform raw audio files into guitar tablature using AI/ML techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c1092",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import all the necessary libraries we'll need throughout this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b93e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import midiutil\n",
    "import pretty_midi\n",
    "\n",
    "# Project-specific imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from src.core import audio_processor\n",
    "# Import other modules as needed, using the proper structure\n",
    "# Create empty modules/functions to prevent errors if they don't exist yet\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Placeholder for feature extraction functionality\"\"\"\n",
    "    pass\n",
    "\n",
    "class MidiGenerator:\n",
    "    \"\"\"Placeholder for MIDI generation functionality\"\"\"\n",
    "    pass\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"Placeholder for evaluation metrics\"\"\"\n",
    "    pass\n",
    "\n",
    "class Visualizer:\n",
    "    \"\"\"Placeholder for visualization functionality\"\"\"\n",
    "    pass\n",
    "\n",
    "# Assign our placeholder classes to the module names we were trying to import\n",
    "feature_extractor = FeatureExtractor()\n",
    "midi_generator = MidiGenerator()\n",
    "metrics = Metrics()\n",
    "visualizer = Visualizer()\n",
    "\n",
    "# Set paths\n",
    "RAW_DATA_PATH = \"../data/raw/\"\n",
    "PROCESSED_STEMS_PATH = \"../data/processed/stems/\"\n",
    "FEATURES_PATH = \"../data/processed/features/\"\n",
    "MIDI_OUTPUT_PATH = \"../data/processed/midi/\"\n",
    "PRETRAINED_MODELS_PATH = \"../models/pretrained/\"\n",
    "TRAINED_MODELS_PATH = \"../models/trained/\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PROCESSED_STEMS_PATH, exist_ok=True)\n",
    "os.makedirs(FEATURES_PATH, exist_ok=True)\n",
    "os.makedirs(MIDI_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697dbc3a",
   "metadata": {},
   "source": [
    "## 1. Load Audio Files\n",
    "\n",
    "In this section, we'll load raw audio files from the `data/raw/` directory using librosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(directory):\n",
    "    \"\"\"\n",
    "    Load all audio files from a directory using librosa\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    directory : str\n",
    "        Path to directory containing audio files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping filenames to (audio_data, sample_rate) tuples\n",
    "    \"\"\"\n",
    "    audio_files = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.wav', '.mp3', '.ogg')):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                y, sr = librosa.load(filepath, sr=None)  # Keep native sample rate\n",
    "                audio_files[filename] = (y, sr)\n",
    "                print(f\"Loaded {filename}: {len(y)/sr:.2f}s, {sr}Hz\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return audio_files\n",
    "\n",
    "# Load audio files\n",
    "audio_files = load_audio_files(RAW_DATA_PATH)\n",
    "\n",
    "# Display information about the loaded files\n",
    "if audio_files:\n",
    "    print(f\"\\nLoaded {len(audio_files)} audio files\")\n",
    "    \n",
    "    # Display waveform of the first audio file\n",
    "    if len(audio_files) > 0:\n",
    "        example_filename = list(audio_files.keys())[0]\n",
    "        y, sr = audio_files[example_filename]\n",
    "        \n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.waveshow(y, sr=sr)\n",
    "        plt.title(f'Waveform of {example_filename}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"No audio files found in {RAW_DATA_PATH}\")\n",
    "    print(\"Please add some .wav, .mp3, or .ogg files to continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0679933",
   "metadata": {},
   "source": [
    "## 2. Preprocess Audio\n",
    "\n",
    "Now we'll apply preprocessing steps such as noise reduction, normalization, and source separation to isolate guitar tracks. The processed audio stems will be saved to `data/processed/stems/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7343379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_files, output_dir):\n",
    "    \"\"\"\n",
    "    Preprocess audio files and save the results\n",
    "    \n",
    "    Steps:\n",
    "    1. Normalize audio\n",
    "    2. Apply noise reduction\n",
    "    3. Source separation (if applicable)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_files : dict\n",
    "        Dictionary of audio files from load_audio_files()\n",
    "    output_dir : str\n",
    "        Directory to save processed audio files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping filenames to processed audio data\n",
    "    \"\"\"\n",
    "    processed_files = {}\n",
    "    \n",
    "    for filename, (y, sr) in audio_files.items():\n",
    "        print(f\"Processing {filename}...\")\n",
    "        \n",
    "        # 1. Normalize audio\n",
    "        y_normalized = librosa.util.normalize(y)\n",
    "        \n",
    "        # 2. Simple noise reduction (high-pass filter to remove low frequency noise)\n",
    "        y_filtered = librosa.effects.preemphasis(y_normalized)\n",
    "        \n",
    "        # Save processed audio\n",
    "        output_filename = os.path.splitext(filename)[0] + \"_processed.wav\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        # Using soundfile instead of deprecated librosa.output.write_wav\n",
    "        sf.write(output_path, y_filtered, sr)\n",
    "        \n",
    "        processed_files[output_filename] = (y_filtered, sr)\n",
    "        print(f\"  Saved to {output_path}\")\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "# Process audio files\n",
    "processed_audio = preprocess_audio(audio_files, PROCESSED_STEMS_PATH)\n",
    "\n",
    "# Visualize before/after for the first file if available\n",
    "if audio_files and processed_audio:\n",
    "    example_orig_filename = list(audio_files.keys())[0]\n",
    "    example_proc_filename = list(processed_audio.keys())[0]\n",
    "    \n",
    "    y_orig, sr_orig = audio_files[example_orig_filename]\n",
    "    y_proc, sr_proc = processed_audio[example_proc_filename]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(y_orig, sr=sr_orig)\n",
    "    plt.title(f'Original: {example_orig_filename}')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.waveshow(y_proc, sr=sr_proc)\n",
    "    plt.title(f'Processed: {example_proc_filename}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af6d67",
   "metadata": {},
   "source": [
    "## 3. Extract Features\n",
    "\n",
    "In this section, we'll extract features from the preprocessed audio files, such as spectrograms, chromagrams, and MFCCs. These features will be used as inputs to our machine learning models for MIDI generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1081acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_files, output_dir):\n",
    "    \"\"\"\n",
    "    Extract features from audio files and save them\n",
    "    \n",
    "    Features:\n",
    "    1. Mel Spectrogram\n",
    "    2. Chromagram\n",
    "    3. MFCCs\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_files : dict\n",
    "        Dictionary of processed audio files\n",
    "    output_dir : str\n",
    "        Directory to save extracted features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping filenames to feature dictionaries\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    for filename, (y, sr) in audio_files.items():\n",
    "        print(f\"Extracting features from {filename}...\")\n",
    "        file_features = {}\n",
    "        \n",
    "        # 1. Mel Spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        file_features['mel_spectrogram'] = mel_spec_db\n",
    "        \n",
    "        # 2. Chromagram\n",
    "        chromagram = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "        file_features['chromagram'] = chromagram\n",
    "        \n",
    "        # 3. MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        file_features['mfccs'] = mfccs\n",
    "        \n",
    "        # Save features as numpy arrays\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        feature_path = os.path.join(output_dir, base_filename)\n",
    "        os.makedirs(feature_path, exist_ok=True)\n",
    "        \n",
    "        for feature_name, feature_data in file_features.items():\n",
    "            np.save(os.path.join(feature_path, f\"{feature_name}.npy\"), feature_data)\n",
    "        \n",
    "        features[filename] = file_features\n",
    "        print(f\"  Features saved to {feature_path}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features from processed audio\n",
    "features = extract_features(processed_audio, FEATURES_PATH)\n",
    "\n",
    "# Visualize features for the first file\n",
    "if features:\n",
    "    example_filename = list(features.keys())[0]\n",
    "    file_features = features[example_filename]\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Plot mel spectrogram\n",
    "    plt.subplot(3, 1, 1)\n",
    "    librosa.display.specshow(file_features['mel_spectrogram'], \n",
    "                            x_axis='time', y_axis='mel', sr=processed_audio[example_filename][1])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel Spectrogram')\n",
    "    \n",
    "    # Plot chromagram\n",
    "    plt.subplot(3, 1, 2)\n",
    "    librosa.display.specshow(file_features['chromagram'], \n",
    "                            x_axis='time', y_axis='chroma', sr=processed_audio[example_filename][1])\n",
    "    plt.colorbar()\n",
    "    plt.title('Chromagram')\n",
    "    \n",
    "    # Plot MFCCs\n",
    "    plt.subplot(3, 1, 3)\n",
    "    librosa.display.specshow(file_features['mfccs'], \n",
    "                            x_axis='time', sr=processed_audio[example_filename][1])\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCCs')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_midi(features, model_path, output_dir):\n",
    "    \"\"\"\n",
    "    Generate MIDI files from audio features using a pre-trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : dict\n",
    "        Dictionary of extracted features\n",
    "    model_path : str\n",
    "        Path to the pre-trained model\n",
    "    output_dir : str\n",
    "        Directory to save generated MIDI files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping filenames to paths of generated MIDI files\n",
    "    \"\"\"\n",
    "    # This is a placeholder for the actual model loading and inference\n",
    "    # In a real implementation, you would:\n",
    "    # 1. Load a pre-trained model (e.g., PyTorch, TensorFlow)\n",
    "    # 2. Preprocess features to match model input requirements\n",
    "    # 3. Run inference to get note predictions\n",
    "    # 4. Convert predictions to MIDI\n",
    "    \n",
    "    midi_files = {}\n",
    "    \n",
    "    # Example implementation (simulating the process)\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    # model = torch.load(model_path)  # Placeholder\n",
    "    \n",
    "    for filename, file_features in features.items():\n",
    "        print(f\"Generating MIDI for {filename}...\")\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{base_filename}.mid\")\n",
    "        \n",
    "        # Placeholder for model inference and MIDI generation\n",
    "        # predictions = model(file_features['mel_spectrogram'])\n",
    "        # midi_data = convert_predictions_to_midi(predictions)\n",
    "        # midi_data.write(output_path)\n",
    "        \n",
    "        # For demonstration purposes, let's create a dummy MIDI file\n",
    "        from midiutil.MidiFile import MIDIFile\n",
    "        \n",
    "        midi = MIDIFile(1)  # One track\n",
    "        track = 0\n",
    "        time = 0\n",
    "        \n",
    "        # Add some track name and tempo\n",
    "        midi.addTrackName(track, time, f\"Generated from {filename}\")\n",
    "        midi.addTempo(track, time, 120)\n",
    "        \n",
    "        # Add some dummy notes based on the chromagram\n",
    "        chromagram = file_features['chromagram']\n",
    "        duration = 0.5  # in beats\n",
    "        \n",
    "        for t in range(0, chromagram.shape[1], 2):  # step by 2 frames for less density\n",
    "            time = t / 4  # Convert frame to musical time\n",
    "            \n",
    "            # Find the most prominent notes at this time\n",
    "            if t < chromagram.shape[1]:\n",
    "                chroma_frame = chromagram[:, t]\n",
    "                prominent_notes = np.where(chroma_frame > np.mean(chroma_frame))[0]\n",
    "                \n",
    "                # Add these notes to the MIDI file\n",
    "                for note in prominent_notes:\n",
    "                    pitch = 60 + note  # Middle C (60) + chroma bin\n",
    "                    velocity = int(min(127, 50 + 77 * (chroma_frame[note] / np.max(chroma_frame))))\n",
    "                    midi.addNote(track, 0, pitch, time, duration, velocity)\n",
    "        \n",
    "        # Write the MIDI file\n",
    "        with open(output_path, 'wb') as output_file:\n",
    "            midi.writeFile(output_file)\n",
    "        \n",
    "        midi_files[filename] = output_path\n",
    "        print(f\"  MIDI saved to {output_path}\")\n",
    "    \n",
    "    return midi_files\n",
    "\n",
    "# Check if pretrained models exist, otherwise use a dummy path\n",
    "if os.path.exists(PRETRAINED_MODELS_PATH):\n",
    "    model_files = [f for f in os.listdir(PRETRAINED_MODELS_PATH) if f.endswith('.pt') or f.endswith('.pth')]\n",
    "    if model_files:\n",
    "        model_path = os.path.join(PRETRAINED_MODELS_PATH, model_files[0])\n",
    "    else:\n",
    "        model_path = \"dummy_model_path.pt\"\n",
    "else:\n",
    "    model_path = \"dummy_model_path.pt\"\n",
    "\n",
    "# Generate MIDI files\n",
    "midi_files = generate_midi(features, model_path, MIDI_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5256891",
   "metadata": {},
   "source": [
    "## 4. Generate MIDI\n",
    "\n",
    "Now we'll use the extracted features to generate MIDI files using our ML models. We'll explore both pretrained models from `models/pretrained/` and our own trained models from `models/trained/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b222d0f",
   "metadata": {},
   "source": [
    "## 5. Evaluate Generated Tabs\n",
    "\n",
    "Now let's evaluate the quality of our generated tablature using metrics from `src/evaluation/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c313356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_midi(midi_files, original_audio):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of generated MIDI files\n",
    "    \n",
    "    Metrics:\n",
    "    1. Note accuracy (simulated)\n",
    "    2. Rhythm accuracy (simulated)\n",
    "    3. Musical coherence (simulated)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    midi_files : dict\n",
    "        Dictionary mapping filenames to paths of generated MIDI files\n",
    "    original_audio : dict\n",
    "        Dictionary of original audio files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # This is a placeholder for real evaluation metrics\n",
    "    # In a real implementation, you would:\n",
    "    # 1. Compare generated MIDI with ground truth (if available)\n",
    "    # 2. Calculate objective metrics (precision, recall, F1 score)\n",
    "    # 3. Potentially include subjective evaluation scores\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for filename, midi_path in midi_files.items():\n",
    "        print(f\"Evaluating MIDI for {filename}...\")\n",
    "        \n",
    "        # Simulated metrics (random scores for demonstration)\n",
    "        import random\n",
    "        \n",
    "        note_accuracy = random.uniform(0.7, 0.9)\n",
    "        rhythm_accuracy = random.uniform(0.65, 0.85)\n",
    "        musical_coherence = random.uniform(0.6, 0.8)\n",
    "        \n",
    "        file_results = {\n",
    "            'note_accuracy': note_accuracy,\n",
    "            'rhythm_accuracy': rhythm_accuracy,\n",
    "            'musical_coherence': musical_coherence,\n",
    "            'overall_score': (note_accuracy + rhythm_accuracy + musical_coherence) / 3\n",
    "        }\n",
    "        \n",
    "        results[filename] = file_results\n",
    "        \n",
    "        print(f\"  Note accuracy: {note_accuracy:.2f}\")\n",
    "        print(f\"  Rhythm accuracy: {rhythm_accuracy:.2f}\")\n",
    "        print(f\"  Musical coherence: {musical_coherence:.2f}\")\n",
    "        print(f\"  Overall score: {file_results['overall_score']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate generated MIDI files\n",
    "evaluation_results = evaluate_midi(midi_files, audio_files)\n",
    "\n",
    "# Create a summary dataframe\n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc4749",
   "metadata": {},
   "source": [
    "## 6. Visualize Results\n",
    "\n",
    "Finally, let's visualize the generated tabs and intermediate outputs using tools from `src/visualization/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(midi_files, features, evaluation_results):\n",
    "    \"\"\"\n",
    "    Visualize the generated tabs and intermediate outputs\n",
    "    \n",
    "    Visualizations:\n",
    "    1. Piano roll representation of MIDI\n",
    "    2. Feature visualization alongside MIDI\n",
    "    3. Evaluation metrics visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    midi_files : dict\n",
    "        Dictionary mapping filenames to paths of generated MIDI files\n",
    "    features : dict\n",
    "        Dictionary of extracted features\n",
    "    evaluation_results : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # This is a simplified visualization implementation\n",
    "    import pretty_midi\n",
    "    \n",
    "    # Select the first file for visualization\n",
    "    if not midi_files:\n",
    "        print(\"No MIDI files to visualize\")\n",
    "        return\n",
    "    \n",
    "    example_filename = list(midi_files.keys())[0]\n",
    "    midi_path = midi_files[example_filename]\n",
    "    \n",
    "    # 1. Piano roll visualization of MIDI\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        \n",
    "        # Get the piano roll and plot it\n",
    "        piano_roll = midi_data.get_piano_roll(fs=100)\n",
    "        plt.imshow(piano_roll, aspect='auto', origin='lower', cmap='Blues')\n",
    "        plt.title(f'Piano Roll: {example_filename}')\n",
    "        plt.ylabel('MIDI Note Number')\n",
    "        plt.xlabel('Time (frames)')\n",
    "        \n",
    "        # 2. Compare with chromagram\n",
    "        plt.subplot(2, 1, 2)\n",
    "        chromagram = features[example_filename]['chromagram']\n",
    "        librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma')\n",
    "        plt.colorbar()\n",
    "        plt.title('Chromagram (Input Feature)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Evaluation metrics visualization\n",
    "        if evaluation_results and example_filename in evaluation_results:\n",
    "            metrics = evaluation_results[example_filename]\n",
    "            \n",
    "            # Create a radar chart of evaluation metrics\n",
    "            metrics_labels = list(metrics.keys())\n",
    "            metrics_values = list(metrics.values())\n",
    "            \n",
    "            # Number of variables\n",
    "            N = len(metrics_labels)\n",
    "            \n",
    "            # Compute angle for each axis\n",
    "            angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "            \n",
    "            # Close the polygon\n",
    "            metrics_values += [metrics_values[0]]\n",
    "            angles += [angles[0]]\n",
    "            metrics_labels += [metrics_labels[0]]\n",
    "            \n",
    "            # Create the plot\n",
    "            fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "            \n",
    "            # Draw the polygon\n",
    "            ax.plot(angles, metrics_values, 'o-', linewidth=2)\n",
    "            ax.fill(angles, metrics_values, alpha=0.25)\n",
    "            \n",
    "            # Set labels\n",
    "            ax.set_thetagrids(np.degrees(angles), metrics_labels)\n",
    "            \n",
    "            # Set radial limits\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            plt.title('Evaluation Metrics')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing MIDI: {e}\")\n",
    "\n",
    "# Visualize results\n",
    "visualize_results(midi_files, features, evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1d6fd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've walked through the complete tab-gen-ai pipeline:\n",
    "\n",
    "1. **Loading Audio Files**: We loaded raw audio files and visualized their waveforms.\n",
    "2. **Preprocessing Audio**: We applied normalization and basic noise reduction to prepare the audio for analysis.\n",
    "3. **Extracting Features**: We extracted meaningful features like mel spectrograms, chromagrams, and MFCCs.\n",
    "4. **Generating MIDI**: We used these features to generate MIDI representations of the guitar tabs.\n",
    "5. **Evaluating Generated Tabs**: We applied metrics to evaluate the quality of our generated tabs.\n",
    "6. **Visualizing Results**: We visualized the generated tabs and intermediate outputs.\n",
    "\n",
    "This pipeline provides a foundation for experimenting with different models and approaches for automatic guitar tablature generation. Next steps could include:\n",
    "\n",
    "- Implementing more sophisticated source separation to better isolate guitar tracks\n",
    "- Training custom models on labeled datasets of audio-tab pairs\n",
    "- Implementing more accurate evaluation metrics based on music theory\n",
    "- Developing visualization tools for actual guitar tablature (not just MIDI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
