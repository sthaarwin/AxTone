{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5825da7d",
   "metadata": {},
   "source": [
    "# Audio Feature Extraction Experiments\n",
    "\n",
    "This notebook explores different techniques for extracting relevant features from audio files that can be used for tablature generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39e7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Set up paths\n",
    "DATA_DIR = '../data/raw/'\n",
    "OUTPUT_DIR = '../data/processed/features/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80429e6f",
   "metadata": {},
   "source": [
    "## Load Sample Audio File\n",
    "\n",
    "Let's load a sample audio file and examine its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f88fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example audio file\n",
    "def load_audio(file_path, sr=22050):\n",
    "    \"\"\"Load an audio file and return the waveform and sample rate\"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    return y, sr\n",
    "\n",
    "# Placeholder - replace with your actual file\n",
    "# file_path = os.path.join(DATA_DIR, 'example_guitar.wav')\n",
    "# y, sr = load_audio(file_path)\n",
    "# print(f'Audio loaded: {len(y)/sr:.2f} seconds, {sr} Hz sample rate')\n",
    "# Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a50a55f",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "We'll implement several feature extraction techniques relevant for guitar tab generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b522295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pitch(y, sr, fmin=80, fmax=800):\n",
    "    \"\"\"Extract pitch information using librosa's pitch tracking\"\"\"\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr, fmin=fmin, fmax=fmax)\n",
    "    return pitches, magnitudes\n",
    "\n",
    "def extract_onsets(y, sr):\n",
    "    \"\"\"Detect note onsets in the audio\"\"\"\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "    return onsets, onset_env\n",
    "\n",
    "def extract_chromagram(y, sr):\n",
    "    \"\"\"Extract chromagram for harmony analysis\"\"\"\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    return chroma\n",
    "\n",
    "def extract_features(y, sr):\n",
    "    \"\"\"Extract all features and return as a dictionary\"\"\"\n",
    "    features = {\n",
    "        'pitches': extract_pitch(y, sr),\n",
    "        'onsets': extract_onsets(y, sr),\n",
    "        'chroma': extract_chromagram(y, sr)\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ef918",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Functions to visualize the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d4dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(y, sr):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_chromagram(chroma, sr):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', sr=sr)\n",
    "    plt.colorbar()\n",
    "    plt.title('Chromagram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_onsets(y, sr, onsets, onset_env):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    times = librosa.times_like(onset_env, sr=sr)\n",
    "    plt.plot(times, onset_env, label='Onset strength')\n",
    "    plt.vlines(librosa.frames_to_time(onsets, sr=sr), 0, onset_env.max(), \n",
    "              color='r', alpha=0.7, linestyle='--', label='Onsets')\n",
    "    plt.legend()\n",
    "    plt.title('Onset Detection')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464ad76",
   "metadata": {},
   "source": [
    "## Feature Extraction Pipeline\n",
    "\n",
    "Now let's put everything together into a pipeline that can be applied to multiple audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afc3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(file_path, output_dir=None):\n",
    "    \"\"\"Process a single audio file and extract features\"\"\"\n",
    "    print(f'Processing: {file_path}')\n",
    "    \n",
    "    # Load audio\n",
    "    y, sr = load_audio(file_path)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(y, sr)\n",
    "    \n",
    "    # Visualize\n",
    "    plot_waveform(y, sr)\n",
    "    plot_chromagram(features['chroma'], sr)\n",
    "    plot_onsets(y, sr, features['onsets'][0], features['onsets'][1])\n",
    "    \n",
    "    # Save features if output_dir is provided\n",
    "    if output_dir:\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_path = os.path.join(output_dir, f'{file_name}_features.npz')\n",
    "        np.savez(output_path, \n",
    "                 pitches=features['pitches'][0],\n",
    "                 pitch_magnitudes=features['pitches'][1],\n",
    "                 onsets=features['onsets'][0],\n",
    "                 onset_env=features['onsets'][1],\n",
    "                 chroma=features['chroma'])\n",
    "        print(f'Features saved to: {output_path}')\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example usage:\n",
    "# file_path = os.path.join(DATA_DIR, 'example_guitar.wav')\n",
    "# features = process_audio_file(file_path, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaeb6f5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on these extracted features, we can move on to:\n",
    "\n",
    "1. Note event detection\n",
    "2. Pitch classification\n",
    "3. String and fret assignment\n",
    "4. Tab notation generation\n",
    "\n",
    "These will be explored in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
